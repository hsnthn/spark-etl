# spark-etl

[![Build Status](https://travis-ci.org/vngrs/spark-etl.svg?branch=master)](https://travis-ci.org/vngrs/spark-etl)
[![Coverage Status](https://coveralls.io/repos/github/vngrs/spark-etl/badge.svg?branch=master)](https://coveralls.io/github/vngrs/spark-etl?branch=master)
[![Join the chat at https://gitter.im/vngrs/spark-etl](https://badges.gitter.im/vngrs/spark-etl.svg)](https://gitter.im/vngrs/spark-etl?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

[The ETL(Extract-Transform-Load)] process is a key component of many data management operations, including move data and to transform the data from one format to another. To effectively support these operations, spark-etl is providig a distributed solution.

spark-etl is a Scala based project and it is developing with Spark. So it is scalable and distributed. spark-etl will process data from N source to N database.
The project structure:
- Extract
  - FILES (json, csv)
  - SQLs
  - NoSQLs
  - Key Value Stores
  - APIs
  - Streams

- Transform
  - Row to json
  - Row to csv
  - Json to Row
  - Change tables
  - Merge tables

- Load
  - FILES (json, csv)
  - SQLs
  - NoSQLs
  - Key Value Stores
  - Elastic Search
  - APIs
  - Streams

Differences between other ETL projects:
  - parallel ETL on cluster level
  - synchronisation
  - open source

Example scenerio to use spark-etl:
We want to get data from multiple sources like MySQL and CVS. When we extracting data, we also want to filter and merge some fields/tables. During the transform layer, we want to run an sql. Then we want to write the transformed data to multiple target like S3 and Redshift.

spark-etl is the easiest way to do this scnereio!

### Version
0.0.1-SNAPSHOT

### Tech
* [Scala] - Functional Programming Language
* [ScalaTest] - ScalaTest is testing tool in the Scala ecosystem.
* [wartremover] - WartRemover is a flexible Scala code linting tool.
* [scalastyle] - Scalastyle examines Scala code and indicates potential problems with it.
* [scoverage] - Scoverage is a code coverage tool for scala that offers statement and branch coverage. * [Apache Spark] - Apache Spark is a fast and general engine for large-scale data processing.
* [travis-ci] - Travis CI is a hosted, distributed continuous integration service used to build and test software projects
* [coveralls] - Coveralls is a web service to help you track your code coverage over time, and ensure that all your new code is fully covered

And of course spark-etl itself is open source with a [public repository][etl]
 on GitHub.

### Installation

Prerequisites for building spark-etl:

- sbt clean assembly

### How to become a committer

Want to contribute? Great!

Committers have write access to the projectâ€™s repositories, i.e., they can modify the code, documentation by themselves and also accept other contributions.

There is no strict protocol for becoming a committer.

Being an active committer means participating on [gitter] discussions, helping to answer questions.
Of course, contributing code and documentation to the project is important as well. A good way to start is contributing improvements, new features, or bug fixes. You need to show that you take responsibility for the code that you contribute, add tests and documentation, and help maintaining it.

If you would like to become a committer, please write on [gitter].

### Development

spark-etl uses Scala and Apache Spark for distributed developing.
The spark-etl committers are using IntelliJ IDEA to develop the spark-etl codebase. We recommend IntelliJ IDEA for developing projects that involve Scala code.

Minimal requirements:

- Support for Scala (Intellij scala plugin)
- Sbt (sbt.version = 0.13.12)

### Todos

 - Scalafmt integration
 - ETL Design

License
----

MIT License

   [etl]: <https://github.com/vngrs/spark-etl>
   [The ETL(Extract-Transform-Load)]: <https://en.wikipedia.org/wiki/Extract,_transform,_load>
   [gitter]: https://gitter.im/vngrs/spark-etl?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
